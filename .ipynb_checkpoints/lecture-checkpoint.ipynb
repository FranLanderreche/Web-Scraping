{
 "metadata": {
  "name": "",
  "signature": "sha256:526f0c8b5a42e9d8020acc1ce9271d13ebadf59ab6afbb0ab1f27822142e2b62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Overview -- Web Scraping\n",
      "\n",
      "We will be using the Wikipedia API to download articles from the web.  Eventually we will be performing NLP and machine learning on the articles to do awesome things like document clustering, document filtering/classification, network analysis, and some indexing.  For this sprint we are just converened with getting some data.\n",
      "\n",
      "## Goals\n",
      "\n",
      "* __Get experience using an [API](http://en.wikipedia.org/wiki/Application_programming_interface) to access [Wikipedia](http://www.wikipedia.org/) articles__\n",
      "* __Store the retrieved articles and metadata in [MongoDB](http://www.mongodb.org/)__ \n",
      "* __Use [regular expressions](http://en.wikipedia.org/wiki/Regular_expression) in [Python](http://docs.python.org/2/howto/regex.html) to search for all articles that contain the word 'Zipf' or 'Zipfian'__\n",
      "* __Augment the article content with contextual information from its [external links](http://en.wikipedia.org/wiki/Wikipedia:External_links)__ \n",
      "* __Have [FUN!](http://media.giphy.com/media/LlmVkDId8FzP2/giphy.gif) (that's an order)__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Resources\n",
      "\n",
      "* __[Requests HTTP library](http://docs.python-requests.org/en/latest/)__\n",
      "* __[Regular expression tester](http://pythex.org/)__\n",
      "* __[Google Regex tutorial](https://developers.google.com/edu/python/regular-expressions)__\n",
      "* __[Beautiful Soup (HTML parsing and searching)](http://www.crummy.com/software/BeautifulSoup/)__\n",
      "* __[MongoDB Python driver](http://api.mongodb.org/python/current/tutorial.html)__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Sources (ranked by ease of use... usually)\n",
      "\n",
      "### DaaS -- Data as a service \n",
      "* time series: [Quandl](http://www.quandl.com/)\n",
      "* public datasets: [enigma](http://enigma.io/)\n",
      "* location contextualization: [factual](http://www.factual.com/)\n",
      "* financial modeling: [Quantopian](https://www.quantopian.com/)\n",
      "* email contextualization: [Rapleaf](http://www.rapleaf.com/why-rapleaf/)\n",
      "* social media: [Gnip](http://gnip.com/)\n",
      "        \n",
      "### Bulk Downloads -- just like the good ol' days\n",
      "\n",
      "* FTP servers\n",
      "* Amazon S3 public [datasets](http://aws.amazon.com/publicdatasets/)\n",
      "* [InfoChimps](http://www.infochimps.com/datasets)\n",
      "* Academia -- [Stanford](http://snap.stanford.edu/data/) and [UCI](http://archive.ics.uci.edu/ml/)\n",
      "\n",
      "### APIs -- public and hidden\n",
      "* [Twitter](https://dev.twitter.com/)\n",
      "* [Foursquare](https://developer.foursquare.com/)\n",
      "* [Facebook](https://developers.facebook.com/docs/reference/apis/)\n",
      "* [Tumblr](http://www.tumblr.com/docs/en/api/v2)\n",
      "* [Rdio](http://developer.rdio.com/)\n",
      "* [Yelp](http://www.yelp.com/developers/documentation)\n",
      "* [Last.fm](http://www.last.fm/api)\n",
      "* [bitly](http://dev.bitly.com/)\n",
      "* [LinkedIn](https://developer.linkedin.com/apis)\n",
      "* [Yahoo Finance (hidden)](http://greenido.wordpress.com/2009/12/22/yahoo-finance-hidden-api/)\n",
      "* [etc.](http://developer.trulia.com/)\n",
      "* [etc.](http://dev.evernote.com/documentation/cloud/)\n",
      "* [etc.](http://www.songkick.com/developer/)\n",
      "\n",
      "### DIY\n",
      "* Webscraping \n",
      "* manual downloads"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise: Wikipedia++"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import pylab to provide scientific Python libraries (NumPy, SciPy, Matplotlib)\n",
      "import pylab\n",
      "\n",
      "# import Python's standard library modules for regular expresions and json\n",
      "import re\n",
      "import json\n",
      "\n",
      "# IF ON Python 3: from urllib.parse import urlparse\n",
      "# This is useful to fix malformed urls \n",
      "import urlparse\n",
      "\n",
      "# import the Image display module\n",
      "from IPython.display import Image\n",
      "\n",
      "# inline allows us to embed matplotlib figures directly into the IPython notebook\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['pylab']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 1: Access the Wikipedia API\n",
      "\n",
      "Lucky for us the Wikipedia [API](http://www.mediawiki.org/wiki/API) is well [documented](http://www.mediawiki.org/wiki/API:FAQ).  And you do not need an API key to access it (isn't that nice of them).  Go ahead, give it a spin!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the Requests HTTP library\n",
      "import requests\n",
      "\n",
      "# A User agent header required for the Wikipedia API.\n",
      "headers = {'user_agent': 'DataWrangling/1.1 (http://zipfianacademy.com; class@zipfianacademy.com)'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment with fetching one or two pages and examining the result (fill in URL and payload)\n",
      "url = 'http://en.wikipedia.org/w/api.php'\n",
      "\n",
      "# parameters for the API request\n",
      "payload = { 'action' : 'parse' , 'format' : 'json','page' : \"Zipf's law\" }\n",
      "\n",
      "# make the request\n",
      "r = requests.post(url, data=payload, headers=headers)\n",
      "\n",
      "# print out the result of the request as JSON\n",
      "r.json();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__HINT: Check out the [parse](http://www.mediawiki.org/wiki/API:Parsing_wikitext#parse) action___"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 2: Persistence in MongoDB\n",
      "\n",
      "Now that you have some experience with the API and can sucessfully access articles with associated metadata, it is time to start storing them in [MongoDB](http://www.mongodb.org/)!\n",
      "\n",
      "You should have a MongoDB [daemon](http://docs.mongodb.org/manual/tutorial/manage-mongodb-processes/) running on your vagrant machine.  It is here that you will be storing all of your data, but be aware of how many articles you are crawling. \n",
      "\n",
      "#### One article = ~120 kilobytes.  500MB / 120KB \u2248 4,250 articles."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import MongoDB modules\n",
      "from pymongo import MongoClient\n",
      "from bson.objectid import ObjectId\n",
      "\n",
      "# connect to the hosted MongoDB instance\n",
      "client = MongoClient('mongodb://localhost:27017/')\n",
      "\n",
      "# connect to the wikipedia database: if it does not exist it will automatically create it -- one reason why mongoDB can be nice.\n",
      "db = client.wikipedia"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each [database](http://docs.mongodb.org/manual/reference/glossary/#term-database) has a number of [collections](http://docs.mongodb.org/manual/reference/glossary/#term-collection) analogous to SQL tables.  And each collection is comprised of [documents](http://docs.mongodb.org/manual/reference/glossary/#term-document) analogous to a rows in a SQL table.  And each document has [fields](http://docs.mongodb.org/manual/reference/glossary/#term-field) analogous to SQL columns.  Also, the docs have made a more comprehensive [comparision](http://docs.mongodb.org/manual/reference/sql-comparison/).\n",
      "\n",
      "![mongo_diagram](http://assets.zipfianacademy.com/data/images/mongo_diagram.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a new collection that is unique to me, this is a necessity since we are all sharing a database \n",
      "collection = db.jdinu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try storing the document you retrieved earlier in MongoDB (be careful to not store duplicates!)\n",
      "if not collection.find_one(r.json()['parse']):\n",
      "    collection.insert(r.json()['parse'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now see if you can query the database for the article you just stored\n",
      "zipf = collection.find_one({ \"title\" : \"Zipf's law\"})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you can store and retrieve articles in the Mongo Database, it is time to iterate!\n",
      "\n",
      "### Step 3: Retrieve and store every article (with associated metadata) within 1 hop from the 'Zipf's law' article.  \n",
      "<b style=\"color: red\">Do not follow external links, only linked Wikipedia articles</b>\n",
      "\n",
      "___HINT: The Zipf's Law article should be located at: 'http://en.wikipedia.org/w/api.php?action=parse&format=json&page=Zipf's%20law'___"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grab the list of linked Wikipedia articles from the API result \n",
      "links = zipf['links']\n",
      "\n",
      "# iterate over each link and store the returned document in MongoDB\n",
      "for link in links:\n",
      "\n",
      "    # parameters for API request\n",
      "    payload = payload = { 'action' : 'parse' , 'format' : 'json','page' : link['*'] }\n",
      "    \n",
      "    r = requests.post(url, data=payload, headers=headers)\n",
      "    \n",
      "    # check to first see if the document is already in our database, if not... store it!\n",
      "    if not collection.find_one(r.json()['parse']):\n",
      "        collection.insert(r.json()['parse'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 3: Find all articles that mention 'Zipf' or 'Zipfian' (case insensitive)\n",
      "\n",
      "We will get some practice now with regular expressions in order to search the content of the articles for the terms `Zipf` or `Zipfian`.  We only want articles that mention these terms in the displayed text however, so we must first remove all the unnecessary HTML tags and only keep what is in between the relevant tags.  Beautiful Soup makes this almost trivial.  Explore the documentation to find how to do this effortlessly: [http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)  \n",
      "\n",
      "Test out your Regular Expressions __before__ you run them over __every__ document you have in your database: [http://pythex.org/](http://pythex.org/).  Here is some useful documentation on regular expressions in Python: [http://docs.python.org/2/howto/regex.html](http://docs.python.org/2/howto/regex.html)\n",
      "        \n",
      "Once you have identified the relevant articles, save them to a file for now, we do not need to persist them in the database.\n",
      "\n",
      "___HINT: There should be 10 articles___"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the Beautiful Soup module \n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "count = 0\n",
      "\n",
      "# compile our regular expression since we will use it many times\n",
      "regex = re.compile('Zipf | Zipfian', re.IGNORECASE)\n",
      "\n",
      "# create an output file if it doesn't already exist, and open it in binary mode\n",
      "out = open('zipfian.txt', 'w+b')\n",
      "\n",
      "# iterate over every document we have stored\n",
      "for doc in collection.find():\n",
      "    # extract the HTML from the document\n",
      "    html = doc['text']['*']\n",
      "\n",
      "    # stringify the ID for serialization to our text file\n",
      "    doc['_id'] = str(doc['_id'])\n",
      "\n",
      "    # create a Beautiful Soup object from the HTML\n",
      "    soup = BeautifulSoup(html)\n",
      "\n",
      "    # extract all the relevant text of the web page: strips out tags and head/meta content\n",
      "    text = soup.get_text()\n",
      "\n",
      "    # perform a regex search with the expression we compiled earlier\n",
      "    m = regex.search(text)\n",
      "\n",
      "    # if our search returned an object (it matched the regex), write the document to our output file\n",
      "    if m:\n",
      "        count += 1\n",
      "        json.dump(doc, out) \n",
      "        out.write('\\n')\n",
      "\n",
      "# close the opened output file for good measure\n",
      "out.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "10"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 4: Augmentation!  Time to remix the web... or rather just Wikipedia. But hey, isn't Wikipedia the web."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to augment our Zipfian Wikipedia articles with content from the __WWW__ at large.  Stepping out of the walled garden of collaboratively edited document safety... let us scrape!  For each of the artcles we found to contain __'Zipf'__ or __'Zipfian'__, we want to know what the web has to say.  For each of the ___external links___ of said articles, fetch the linked webpage and extract the __&lt;title&gt;__ and __&lt;meta name=\"keywords\"&gt;__ from the __HTML__.  Beautiful Soup would probably help you a lot here. \n",
      "\n",
      "___You still have to watch out for pages without keywords or a title___\n",
      "\n",
      "Once you have extracted this information, update the stored document in your database with this information. Add a field called __'extraexternal'__ that contains the additional contextual information.  __'extraexternal'__ should be an array of __JSON__ objects, each of which have keys:\n",
      "    \n",
      "* __'url'__ : the url of the page\n",
      "* __'title'__ : the title of the page\n",
      "* __'keywords'__ : the keywords from the meta tag\n",
      "    \n",
      "__Example:__\n",
      "    \n",
      "        {\n",
      "         \n",
      "         ...\n",
      "         \n",
      "         'displaytitle': \"Zipf's law\",\n",
      "         'externallinks': [...],\n",
      "         'text': {\n",
      "                    '*': '<table class=\"infobox bordered\" style=\"width:325px; max-width:325px; font-size:95%; text-align: left;\">\\n<caption>Zipf\\'s law</caption>\\n<tr...'\n",
      "                  }\n",
      "        'extraexternal' : [{ \n",
      "                             'url' : 'http://zipfianacademy.com',\n",
      "                             'title' : 'Teaching the Long Tail | Zipfian Academy'\n",
      "                             'keywords' : 'data, datascience, science, bootcamp, training, hadoop, big, bigdata, boot, camp, machine...'\n",
      "                           }, ... ]\n",
      "         ...\n",
      "        } \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# re-open our output file of matched articles \n",
      "articles = open('zipfian.txt', 'r')\n",
      "\n",
      "# iterate over each article that contains 'Zipf' or 'Zipfian'\n",
      "for line in articles:\n",
      "    doc = json.loads(line)\n",
      "\n",
      "    # extract the external links from the Wikipedia article\n",
      "    links = doc['externallinks']\n",
      "\n",
      "    # deserialize our document ID into a Mongo ObjectID\n",
      "    _id = ObjectId(doc['_id'])\n",
      "\n",
      "    # create an empty 'extraexternal' array to store the results of our web scraping\n",
      "    collection.update( { '_id' : _id }, { '$set' :  { 'extraexternal' : [] } } )\n",
      "\n",
      "    # iterate over the URLs of the external links\n",
      "    for url in links:\n",
      "        # sometimes the URLs are malformed, split the URL into its component parts to fix\n",
      "        scheme, netloc, path, qs, anchor = urlparse.urlsplit(url)\n",
      "\n",
      "        # if there is not a scheme specified (what comes before the ://) default to HTTP\n",
      "        scheme = scheme if scheme else 'http'\n",
      "\n",
      "        # rejoin the fixed components into a URL string\n",
      "        fixed_url = urlparse.urlunsplit((scheme, netloc, path, qs, anchor))\n",
      "\n",
      "        # make the request to grab the content of the external link\n",
      "        html = requests.get(fixed_url)\n",
      "\n",
      "        # soupify the HTML so we can traverse/parse it for what we are looking for\n",
      "        soup = BeautifulSoup(html.text)\n",
      "\n",
      "        # extract the title from the page\n",
      "        title = soup.title\n",
      "\n",
      "        # extract the keywords from the meta tag\n",
      "        keywords = soup.find('meta', attrs={'name' : 'keywords'})\n",
      "\n",
      "        # create a object to store our extra data related to each external link\n",
      "        augment = { \n",
      "                    'url' : url, \n",
      "                    'title' : title.string if title else \"\", \n",
      "                    'keywords' : keywords['content'].split(',') if keywords else [] \n",
      "                  }\n",
      "\n",
      "        # update the document we already have stored in MongoDB with our additional information\n",
      "        collection.update({ '_id' : _id }, { '$push' : { 'extraexternal' : augment } } )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint as pp\n",
      "\n",
      "for doc in collection.find({'extraexternal' : { '$exists' :  True } }):\n",
      "    pp.pprint(doc['title'])\n",
      "    pp.pprint(doc['extraexternal'])\n",
      "    print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "u'George Kingsley Zipf'\n",
        "[{u'keywords': [u'jan wassenaar',\n",
        "                u' 2dcurves',\n",
        "                u' mathematical curves',\n",
        "                u' hyperbola'],\n",
        "  u'title': u'hyperbola',\n",
        "  u'url': u'http://www.2dcurves.com/conicsection/conicsectionh.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'RAM-Verlag  \\r\\n\\r\\nRAM-Verlag Qantitative Linguistics',\n",
        "  u'url': u'http://www.ram-verlag.de/g3inh.htm'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n    Zipf Dies After 3 - Month Illness |\\n    \\n        News |\\n    \\n    The Harvard Crimson\\n',\n",
        "  u'url': u'http://www.thecrimson.com/article.aspx?ref=204013'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.bibleplants.com/polaris/1919.htm#zipf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'VIAF',\n",
        "  u'url': u'http://viaf.org/viaf/79156155'}]\n",
        "\n",
        "\n",
        "u'Zeta distribution'\n",
        "[{u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.3284&rep=rep1&type=pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Zipf Distribution -- from Wolfram MathWorld',\n",
        "  u'url': u'http://mathworld.wolfram.com/ZipfDistribution.html'}]\n",
        "\n",
        "\n",
        "u'Brown Corpus'\n",
        "[{u'keywords': [],\n",
        "  u'title': u'The Linguistics Encyclopedia - Google Books',\n",
        "  u'url': u'http://books.google.com.au/books?id=IG7tE4-p-uUC&pg=PA87'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Brown Corpus Manual',\n",
        "  u'url': u'http://khnt.aksis.uib.no/icame/manuals/brown/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'The Brown Corpus Tag-set',\n",
        "  u'url': u'http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Natural Language Toolkit \\u2014 NLTK 2.0 documentation',\n",
        "  u'url': u'http://www.nltk.org/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Part Of Speech Tagging - PHP/ir',\n",
        "  u'url': u'http://phpir.com/part-of-speech-tagging'}]\n",
        "\n",
        "\n",
        "u'Principle of least effort'\n",
        "[]\n",
        "\n",
        "\n",
        "u'Rank-size distribution'\n",
        "[{u'keywords': [],\n",
        "  u'title': u'404 Not Found',\n",
        "  u'url': u'http://people.few.eur.nl/vanmarrewijk/geography/zipf/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www-personal.umich.edu/~copyrght/image/monog08/fulltext.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://cep.lse.ac.uk/pubs/download/dp0641.pdf'},\n",
        " {u'keywords': [u''],\n",
        "  u'title': u'Oxford University Press | Online Resource Centre | Online Resource Centres',\n",
        "  u'url': u'http://www.oup.com/uk/orc/bin/9780199280988/01student/zipf/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'The Return of Zipf: Towards a Further Understanding of the Rank-Size Distribution - Brakman - 2002 - Journal of Regional Science - Wiley Online Library',\n",
        "  u'url': u'http://dx.doi.org/10.1111%2F1467-9787.00129'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Rank-Size Distribution and the Process of Urban Growth ',\n",
        "  u'url': u'http://dx.doi.org/10.1080%2F00420989550012960'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n The Pareto, Zipf and other power laws\\n ',\n",
        "  u'url': u'http://dx.doi.org/10.1016%2FS0165-1765(01)00524-9'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Page not found - Mark Kimura, Ph.D. Official Site',\n",
        "  u'url': u'http://www.mkimura.com/wikipedia/UseOfABM_RS.pdf'}]\n",
        "\n",
        "\n",
        "u'List of probability distributions'\n",
        "[]\n",
        "\n",
        "\n",
        "u'Zipf\\u2013Mandelbrot law'\n",
        "[{u'keywords': [],\n",
        "  u'title': u'Introduction of relative abundance distribution (RAD) indices, estimated from the rank-frequency diagrams (RFD), to assess changes in community diversity',\n",
        "  u'url': u'http://cat.inist.fr/?aModele=afficheN&cpsidt=1411186'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Introduction of Relative Abundance Distribution (RAD) Indices, Estimated from the Rank-Frequency Diagrams (RFD), to Assess Changes in Community Diversity - Springer',\n",
        "  u'url': u'http://dx.doi.org/10.1023%2FA:1006297211561'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'csKw:writings:computer:webcam',\n",
        "  u'url': u'http://shaunwagner.com/writings_computer_evomus.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"[physics/9901035] Citations and the Zipf-Mandelbrot's law\",\n",
        "  u'url': u'http://arxiv.org/abs/physics/9901035'},\n",
        " {u'keywords': [u\"Zipf's law\"],\n",
        "  u'title': u\"Zipf's law\",\n",
        "  u'url': u'http://www.nist.gov/dads/HTML/zipfslaw.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'404 - Not Found',\n",
        "  u'url': u'http://www.nslij-genetics.org/wli/zipf/index.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Zipf Law Peculiarities for Dictionary Definitions',\n",
        "  u'url': u'http://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm'}]\n",
        "\n",
        "\n",
        "u'Pareto principle'\n",
        "[{u'keywords': [u'Juran  Joseph M',\n",
        "                u'Executives and Management',\n",
        "                u'Quality Control Handbook (Book)',\n",
        "                u'Deaths (Obituaries)',\n",
        "                u'New York State'],\n",
        "  u'title': u'Joseph Juran, 103, Pioneer in Quality Control, Dies - New York Times',\n",
        "  u'url': u'http://www.nytimes.com/2008/03/03/business/03juran.html'},\n",
        " {u'keywords': [u'Management Guru',\n",
        "                u' 80/20 rule',\n",
        "                u' Pareto\\x92s law',\n",
        "                u' pareto principle',\n",
        "                u' the 80 20 principle',\n",
        "                u' effective speaking',\n",
        "                u' the 80 20 rule',\n",
        "                u' 80 20 principle',\n",
        "                u' 80 20 rule',\n",
        "                u' keynote address',\n",
        "                u' Consulting Service',\n",
        "                u' second opinion',\n",
        "                u' second opinion services',\n",
        "                u' Conference presentation',\n",
        "                u' Seminar presentation',\n",
        "                u' Report writing',\n",
        "                u' Written communication',\n",
        "                u' Presentation training',\n",
        "                u' Presentation skills',\n",
        "                u' Presentation skills training'],\n",
        "  u'title': u'What is 80/20 Rule, Pareto\\x92s Law, Pareto Principle,  The 80 20 Principle, Written communication, Presentation skills',\n",
        "  u'url': u'http://www.80-20presentationrule.com/whatisrule.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://arxiv.org/PS_cache/cond-mat/pdf/0412/0412004v3.pdf'},\n",
        " {u'keywords': [u'The Forbes 100 billionaire rich list'],\n",
        "  u'title': u'The Forbes top 100 billionaire rich-list  | This is Money',\n",
        "  u'url': u'http://www.thisismoney.co.uk/news/article.html?in_article_id=418243&in_page_id=3'},\n",
        " {u'keywords': [u'human development report',\n",
        "                u' undp',\n",
        "                u' united nations development programme',\n",
        "                u' hdr',\n",
        "                u' capabilities approach',\n",
        "                u' nhdr',\n",
        "                u' hdi',\n",
        "                u' human development index',\n",
        "                u' gem',\n",
        "                u' gender empowerment measure',\n",
        "                u' gdi',\n",
        "                u' gender-related development index',\n",
        "                u' hpi',\n",
        "                u' human poverty index',\n",
        "                u' human development report office',\n",
        "                u' hdro',\n",
        "                u' un',\n",
        "                u' international development',\n",
        "                u' national development report',\n",
        "                u' regional development report',\n",
        "                u' journal of human development',\n",
        "                u' development paradigm',\n",
        "                u' homepage',\n",
        "                u' development policies',\n",
        "                u' development theory',\n",
        "                u' social development',\n",
        "                u' cultural development',\n",
        "                u' economic development',\n",
        "                u' economic growth theory',\n",
        "                u' social progress',\n",
        "                u' social justice',\n",
        "                u' development as freedom',\n",
        "                u' human rights approach',\n",
        "                u' capability approach',\n",
        "                u' human development approach',\n",
        "                u' amartya sen',\n",
        "                u' mahbub ul haq',\n",
        "                u' human mobility',\n",
        "                u' fighting climate change',\n",
        "                u' water scarcity',\n",
        "                u' Aid trade and security',\n",
        "                u' human security',\n",
        "                u' Millennium Development Goals',\n",
        "                u' MDGs',\n",
        "                u' gender inequality',\n",
        "                u' citizen s participation in development',\n",
        "                u' participatory development',\n",
        "                u' participatory approach',\n",
        "                u' National and international strategies for development',\n",
        "                u' Concepts and measurements of development',\n",
        "                u' national statistics',\n",
        "                u' global statistics',\n",
        "                u' measuring development',\n",
        "                u' measuring progress',\n",
        "                u' statistical data',\n",
        "                u''],\n",
        "  u'title': u'Reports (1990-2013) | Global Reports | HDR 1992 | Download | Human Development Reports (HDR) | United Nations Development Programme (UNDP)',\n",
        "  u'url': u'http://hdr.undp.org/en/reports/global/hdr1992/chapters/'},\n",
        " {u'keywords': [u'.NET ', u' Security'],\n",
        "  u'title': u\"Microsoft's CEO: 80-20 Rule Applies To Bugs, Not Just Features\",\n",
        "  u'url': u'http://www.crn.com/news/security/18821726/microsofts-ceo-80-20-rule-applies-to-bugs-not-just-features.htm'},\n",
        " {u'keywords': [u''],\n",
        "  u'title': u'\\n\\nKathryn Woodcock - Ryerson University\\n',\n",
        "  u'url': u'http://www.ryerson.ca/woodcock/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.uscg.mil/hq/cg5/cg5211/docs/RBDM_Files/PDF/RBDM_Guidelines/Volume%202/Volume%202-Chapter%206.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Growing artificial societies [electronic resource]: social science from the ... - Joshua M. Epstein - Google Books',\n",
        "  u'url': u'http://books.google.com/?id=xXvelSs2caQC'},\n",
        " {u'keywords': [u'Providence',\n",
        "                u' Breaking News',\n",
        "                u' RI Talks',\n",
        "                u' Blogs',\n",
        "                u' Education',\n",
        "                u' Environment',\n",
        "                u' Health',\n",
        "                u' Police & Fire',\n",
        "                u' Politics',\n",
        "                u' Sports',\n",
        "                u' Business',\n",
        "                u' Arts',\n",
        "                u' Entertainment',\n",
        "                u' Events',\n",
        "                u' Cars',\n",
        "                u' Real Estate',\n",
        "                u' Jobs',\n",
        "                u' Obituaries',\n",
        "                u' Weather',\n",
        "                u' Traffic',\n",
        "                u' Markets',\n",
        "                u' Lotteries'],\n",
        "  u'title': u'\\nProvidence Journal | Rhode Island news, sports, weather & more - The Providence Journal\\n',\n",
        "  u'url': u'http://www.projo.com/opinion/contributors/content/CT_weinberg27_07-27-09_HQF0P1E_v15.3f89889.html'},\n",
        " {u'keywords': [u'girls',\n",
        "                u' chubby bunny',\n",
        "                u' marshmallows',\n",
        "                u' cute',\n",
        "                u' pretty',\n",
        "                u' adorable',\n",
        "                u' funny girls',\n",
        "                u' crazy girls'],\n",
        "  u'title': u'three cute girls do the chubby bunny challenge - YouTube',\n",
        "  u'url': u'http://howdoigetoffdrugs.com/2010/01/career-criminals-who-are-they-and-what-should-society-do-about-them/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.dmacorporation.com/dmarecognized/MortgagePress.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'On Line Calculator: Inequality',\n",
        "  u'url': u'http://www.poorcity.richcity.org/calculator/?quantiles=82.4,17.6%7C17.6,82.4'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Informetric distributions, part I: Unified overview - Bookstein - 1999 - Journal of the American Society for Information Science - Wiley Online Library',\n",
        "  u'url': u'http://dx.doi.org/10.1002%2F(SICI)1097-4571(199007)41:5%3C368::AID-ASI8%3E3.0.CO%3B2-C'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n The Forbes 400 and the Pareto wealth distribution\\n ',\n",
        "  u'url': u'http://dx.doi.org/10.1016%2Fj.econlet.2005.08.020'},\n",
        " {u'keywords': [u'Business',\n",
        "                u' success',\n",
        "                u' pareto',\n",
        "                u' principle',\n",
        "                u' 8020',\n",
        "                u' businesseconomics'],\n",
        "  u'title': u'The 80-20 Principle The Secret to Success by Achieving More with Less',\n",
        "  u'url': u'http://www.scribd.com/doc/3664882/The-8020-Principle-The-Secret-to-Success-by-Achieving-More-with-Less'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n The Pareto, Zipf and other power laws\\n ',\n",
        "  u'url': u'http://dx.doi.org/10.1016%2FS0165-1765(01)00524-9'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n The size distribution of cities: An examination of the Pareto law and primacy\\n ',\n",
        "  u'url': u'http://dx.doi.org/10.1016%2F0094-1190(80)90043-1'},\n",
        " {u'keywords': [u'pareto principle principal law 80/20 80-20 80 20 rule dr joseph juran vital few and trivial many superstar management theory'],\n",
        "  u'title': u\"Pareto's Principle - The 80-20 Rule\",\n",
        "  u'url': u'http://management.about.com/cs/generalmanagement/a/Pareto081202.htm'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://web.archive.org/web/20080528063231/http://www.ma.hw.ac.uk/~des/HWM00-26.pdf'}]\n",
        "\n",
        "\n",
        "u\"Zipf's law\"\n",
        "[{u'keywords': [],\n",
        "  u'title': u'Zipf, Power-law, Pareto - a ranking tutorial',\n",
        "  u'url': u'http://www.hpl.hp.com/research/idl/papers/ranking/ranking.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'404 - Not Found',\n",
        "  u'url': u'http://www.nslij-genetics.org/wli/pub/ieee92_pre.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'IEEE Xplore - \\r\\n\\t\\tRandom texts exhibit Zipf&#39;s-law-like word frequency distribution\\r\\n\\t',\n",
        "  u'url': u'http://dx.doi.org/10.1109%2F18.165464'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"Peter Neumann's Home Page\",\n",
        "  u'url': u'http://www.csl.sri.com/users/neumann/#12a'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'WebCite query result',\n",
        "  u'url': u'http://www.webcitation.org/5z2UByabR'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Least effort and the origins of scaling in human language ',\n",
        "  u'url': u'http://www.pnas.org/content/100/3/788.abstract?sid=cc7fae18-87c9-4b67-863a-4195bb47c1d1'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Least effort and the origins of scaling in human language ',\n",
        "  u'url': u'http://dx.doi.org/10.1073%2Fpnas.0335980100'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Least effort and the origins of scaling in human language',\n",
        "  u'url': u'//www.ncbi.nlm.nih.gov/pmc/articles/PMC298679'},\n",
        " {u'keywords': [u'PubMed',\n",
        "                u' National Center for Biotechnology Information',\n",
        "                u' NCBI',\n",
        "                u' United States National Library of Medicine',\n",
        "                u' NLM',\n",
        "                u' MEDLINE',\n",
        "                u' Medical Journals',\n",
        "                u' pub med',\n",
        "                u' Entrez',\n",
        "                u' Journal Articles',\n",
        "                u' Citation search'],\n",
        "  u'title': u'Least effort and the origins of sca... [Proc Natl Acad Sci U S A. 2003] - PubMed - NCBI',\n",
        "  u'url': u'//www.ncbi.nlm.nih.gov/pubmed/12540826'},\n",
        " {u'keywords': [u'factorial randomness',\n",
        "                u\" Zipf's Law\",\n",
        "                u\" Benford's Law\",\n",
        "                u' factors',\n",
        "                u' natural numbers',\n",
        "                u' random',\n",
        "                u' randomness',\n",
        "                u' chi-square',\n",
        "                u' linear regression',\n",
        "                u' PASCAL',\n",
        "                u' lemma',\n",
        "                u' correlation coefficient',\n",
        "                u' benford',\n",
        "                u' zipf'],\n",
        "  u'title': u'Factorial Randomness',\n",
        "  u'url': u'http://home.zonnet.nl/galien8/factor/factor.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Zipf Law Peculiarities for Dictionary Definitions',\n",
        "  u'url': u'http://www.gelbukh.com/CV/Publications/2001/CICLing-2001-Zipf.htm'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"[cs/0406015] Zipf's law and the creation of musical context\",\n",
        "  u'url': u'http://xxx.arxiv.org/abs/cs.CL/0406015'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://pages.stern.nyu.edu/~xgabaix/papers/zipf.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"Zipf's Law for Cities: An Explanation \",\n",
        "  u'url': u'http://dx.doi.org/10.1162%2F003355399556133'},\n",
        " {u'keywords': [u'Makro\\xc3\\x83\\xc2\\xb8konomi',\n",
        "                u' mikro\\xc3\\x83\\xc2\\xb8konomi',\n",
        "                u' makro\\xc3\\x83\\xc2\\xb8konomiske tidsskrifter.'],\n",
        "  u'title': u'Quarterly journal of economics. (eJournal / eMagazine) [WorldCat.org]',\n",
        "  u'url': u'//www.worldcat.org/issn/0033-5533'},\n",
        " {u'keywords': [u''],\n",
        "  u'title': u'Guest Column: Math and the City - NYTimes.com',\n",
        "  u'url': u'http://judson.blogs.nytimes.com/2009/05/19/math-and-the-city/'},\n",
        " {u'keywords': [u'The Atlantic',\n",
        "                u' The Atlantic Magazine',\n",
        "                u' TheAtlantic.com',\n",
        "                u' Atlantic',\n",
        "                u' news',\n",
        "                u' opinion',\n",
        "                u' breaking news',\n",
        "                u' analysis',\n",
        "                u' commentary',\n",
        "                u' business',\n",
        "                u' politics',\n",
        "                u' culture',\n",
        "                u' international',\n",
        "                u' science',\n",
        "                u' technology',\n",
        "                u' national and life'],\n",
        "  u'title': u'Seeing Around Corners - Jonathan Rauch - The Atlantic',\n",
        "  u'url': u'http://www.theatlantic.com/issues/2002/04/rauch.htm'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"Zipf's law | planetmath.org\",\n",
        "  u'url': u'http://planetmath.org/encyclopedia/ZipfsLaw.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Comptes Rendus de l\\x92Acad\\xe9mie des Sciences',\n",
        "  u'url': u'http://www.hubbertpeak.com/laherrere/fractal.htm'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Sign in to read: Why it is hard to share the wealth - physics-math - 12 March 2005 - New Scientist',\n",
        "  u'url': u'http://www.newscientist.com/article.ns?id=mg18524904.300'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.lexique.org/listes/liste_mots.txt'},\n",
        " {u'keywords': [u'mobile',\n",
        "                u' pi theorem',\n",
        "                u' php',\n",
        "                u' ajax',\n",
        "                u' semantic web',\n",
        "                u' india',\n",
        "                u' philosophy'],\n",
        "  u'title': u'Semantic Depth Analyzer',\n",
        "  u'url': u'http://1.1o1.in/en/webtools/semantic-depth'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"[physics/9901035] Citations and the Zipf-Mandelbrot's law\",\n",
        "  u'url': u'http://uk.arxiv.org/abs/physics/9901035'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Wolfram Demonstrations Project',\n",
        "  u'url': u'http://demonstrations.wolfram.com/ZipfsLawForUSCities/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"Zipf's Law -- from Wolfram MathWorld\",\n",
        "  u'url': u'http://mathworld.wolfram.com/ZipfsLaw.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.geoffkirby.co.uk/ZIPFSLAW.pdf'},\n",
        " {u'keywords': [u' Mathematics', u' Statistics', u' Physics'],\n",
        "  u'title': u\"\\n\\t\\t\\tComplex systems: Unzipping Zipf's law :  Nature :  Nature Publishing Group\\n\\t\\t\",\n",
        "  u'url': u'http://www.nature.com/nature/journal/v474/n7350/full/474164a.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'CiteSeerX',\n",
        "  u'url': u'http://citeseer.ist.psu.edu/context/64879/0'}]\n",
        "\n",
        "\n",
        "u'Stable distribution'\n",
        "[{u'keywords': [],\n",
        "  u'title': u'Generalization of symmetric \\u03b1-stable L\\xe9vy distributions for q>1',\n",
        "  u'url': u'http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2869267'},\n",
        " {u'keywords': [],\n",
        "  u'title': u\"[0911.2009] Generalization of symmetric $\\\\alpha$-stable L\\\\'evy distributions for\\n  $q&gt;1$\",\n",
        "  u'url': u'http://arxiv.org/abs/0911.2009'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Generalization of symmetric alpha-stable L\\xe9vy distributions for q>1',\n",
        "  u'url': u'http://adsabs.harvard.edu/abs/2010JMP....51c3502U'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Generalization of symmetric \\u03b1-stable L\\xe9vy distributions for q>1 | J. Math. Phys. - Journal of Mathematical Physics',\n",
        "  u'url': u'http://dx.doi.org/10.1063%2F1.3305292'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Generalization of symmetric \\u03b1-stable L\\xe9vy distributions for q>1',\n",
        "  u'url': u'//www.ncbi.nlm.nih.gov/pmc/articles/PMC2869267'},\n",
        " {u'keywords': [u'PubMed',\n",
        "                u' National Center for Biotechnology Information',\n",
        "                u' NCBI',\n",
        "                u' United States National Library of Medicine',\n",
        "                u' NLM',\n",
        "                u' MEDLINE',\n",
        "                u' Medical Journals',\n",
        "                u' pub med',\n",
        "                u' Entrez',\n",
        "                u' Journal Articles',\n",
        "                u' Citation search'],\n",
        "  u'title': u'Generalization of symmetric alpha-stable L\\xe9vy di... [J Math Phys. 2010] - PubMed - NCBI',\n",
        "  u'url': u'//www.ncbi.nlm.nih.gov/pubmed/20596232'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n        Taylor & Francis Online\\n         :: Theory of the pressure broadening and shift of spectral lines - Advances in Physics - Volume 30,\\n        Issue 3 \\n    ',\n",
        "  u'url': u'http://journalsonline.tandf.co.uk/openurl.asp?genre=article&eissn=1460-6976&volume=30&issue=3&spage=367'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Theory of the pressure broadening and shift of spectral lines.',\n",
        "  u'url': u'http://adsabs.harvard.edu/abs/1981AdPhy..30..367P'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\n        Taylor & Francis Online\\n         :: Theory of the pressure broadening and shift of spectral lines - Advances in Physics - Volume 30,\\n        Issue 3 \\n    ',\n",
        "  u'url': u'http://dx.doi.org/10.1080%2F00018738100101467'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://www.fel.duke.edu/~scafetta/pdf/opinion0308.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Page not found (Error 404) | University of North Texas Libraries',\n",
        "  u'url': u'http://www.library.unt.edu/theses/open/20013/leddon_deborah/dissertation.pdf'},\n",
        " {u'keywords': [u'one-dimensional strongly stable laws',\n",
        "                u' multivariate spherically symmetric stable distributions',\n",
        "                u' higher transcendental functions',\n",
        "                u' generalized hypergeometrical series',\n",
        "                u' Meijer\\u2019s G-functions'],\n",
        "  u'title': u'\\n    \\n            On Representation of Densities of Stable Laws by Special Functions : Theory of Probability & Its Applications: Vol. 39, No. 2\\n        \\n    (Society for Industrial and Applied Mathematics)\\n',\n",
        "  u'url': u'http://epubs.siam.org/tvp/resource/1/tprbau/v39/i2/p354_s1'},\n",
        " {u'keywords': [u'one-dimensional strongly stable laws',\n",
        "                u' multivariate spherically symmetric stable distributions',\n",
        "                u' higher transcendental functions',\n",
        "                u' generalized hypergeometrical series',\n",
        "                u' Meijer\\u2019s G-functions'],\n",
        "  u'title': u'\\n    \\n            On Representation of Densities of Stable Laws by Special Functions : Theory of Probability & Its Applications: Vol. 39, No. 2\\n        \\n    (Society for Industrial and Applied Mathematics)\\n',\n",
        "  u'url': u'http://dx.doi.org/10.1137%2F1139025'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Continuous and discrete properties of stochastic processes - Nottingham eTheses',\n",
        "  u'url': u'http://etheses.nottingham.ac.uk/1194/'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://academic2.american.edu/~jpnolan/stable/chap1.pdf'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'',\n",
        "  u'url': u'http://arxiv.org/pdf/math/0408321v2'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\nWiley: The Best of Wilmott 1: Incorporating the Quantitative Finance Review - Paul Wilmott\\n\\n   ',\n",
        "  u'url': u'http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470023511,descCd-tableOfContents.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'Page not found | planetmath.org',\n",
        "  u'url': u'http://planetmath.org/encyclopedia/StrictlyStableRandomVariable.html'},\n",
        " {u'keywords': [u'stable distribution',\n",
        "                u' heavy tails',\n",
        "                u' pareto',\n",
        "                u' stable law'],\n",
        "  u'title': u\"John Nolan's Stable Distribution Page\",\n",
        "  u'url': u'http://academic2.american.edu/~jpnolan/stable/stable.html'},\n",
        " {u'keywords': [],\n",
        "  u'title': u'\\r\\n  Overview\\r\\n ',\n",
        "  u'url': u'http://www.mathestate.com/tools/Financial/map/Overview.html'}]\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Congratulations!\n",
      "\n",
      "![won-the-internet](http://25.media.tumblr.com/tumblr_m8bg80KH5l1qlh1s6o1_400.gif)\n",
      "\n",
      "#### You have made it to the end (hopefully succcessfully).  Now that you have your data and have contextualized it with information from the web, you can start performing some interesting analyses on it. Some ideas to get you started:\n",
      "\n",
      "* [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_Classification) document clasification using [bag of words](http://en.wikipedia.org/wiki/Bag-of-words_model) vectorization\n",
      "* [Six Degrees of Wikipedia](http://en.wikipedia.org/wiki/Wikipedia:Six_degrees_of_Wikipedia) graph traversal \n",
      "* Other awesome things!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}